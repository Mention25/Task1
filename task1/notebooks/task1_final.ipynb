{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12773518,"sourceType":"datasetVersion","datasetId":8075275},{"sourceId":12773525,"sourceType":"datasetVersion","datasetId":8075280},{"sourceId":13116883,"sourceType":"datasetVersion","datasetId":8309178}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport torch\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-23T08:24:01.942733Z","iopub.status.idle":"2025-09-23T08:24:01.943069Z","shell.execute_reply.started":"2025-09-23T08:24:01.942928Z","shell.execute_reply":"2025-09-23T08:24:01.942944Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#读入数据\nimport pandas as pd\ntrain_df = pd.read_csv(\"/kaggle/input/train-tsv/new_train.tsv\",\n                       sep=\"\\t\", names=['text', 'label']) \ntest_df = pd.read_csv(\"/kaggle/input/test-tsv/new_test.tsv\", \n                       sep='\\t', names=['text', 'label'])\ntrain_df.head()\ntest_df.head()\n\n#划分数据集\nfrom sklearn.model_selection import train_test_split\ntrain_texts, valid_texts, train_labels, valid_labels = train_test_split(\n    train_df[\"text\"], train_df[\"label\"], test_size=0.2, random_state=42\n)\n\n#文本预处理\nimport re\ndef preprocess_text(text):\n    text = text.lower()\n    text = re.sub(r'[^a-z\\s]', '', text)\n    return text.split()\n\n# ----------- BoW 方法 ------------\ndef build_bow(sentences, vocab=None):\n    # 如果没有传 vocab，说明是训练集，需要自己构建\n    if vocab is None:\n        vocab = {}\n        for sent in sentences:\n            tokens = preprocess_text(sent)\n            for word in tokens:\n                if word not in vocab:\n                    vocab[word] = len(vocab)\n\n    bow_matrix = torch.zeros(len(sentences), len(vocab), dtype=torch.float32)\n    for i, sent in enumerate(sentences):\n        tokens = preprocess_text(sent)\n        for word in tokens:\n            if word in vocab:  # 只统计在 vocab 里的词\n                bow_matrix[i][vocab[word]] += 1\n\n    return bow_matrix, vocab\n\n\n# ----------- N-gram 方法 ------------\ndef generate_ngrams(tokens, n=2):\n    \"\"\"\n    从分词列表生成 n-grams，例如 bigram (n=2)\n    ['this', 'is', 'good'] -> ['this is', 'is good']\n    \"\"\"\n    if len(tokens) < n:\n        return []\n    ngrams = zip(*[tokens[i:] for i in range(n)])\n    return [\" \".join(ngram) for ngram in ngrams]\n    \ndef build_ngram_bow(sentences, n=2, vocab=None):\n    if vocab is None:\n        vocab = {}\n        for sent in sentences:\n            tokens = preprocess_text(sent)\n            ngrams = generate_ngrams(tokens, n)\n            features = tokens + ngrams\n            for word in features:\n                if word not in vocab:\n                    vocab[word] = len(vocab)\n\n    bow_matrix = torch.zeros(len(sentences), len(vocab), dtype=torch.float32)\n    for i, sent in enumerate(sentences):\n        tokens = preprocess_text(sent)\n        ngrams = generate_ngrams(tokens, n)\n        features = tokens + ngrams\n        for word in features:\n            if word in vocab:\n                bow_matrix[i][vocab[word]] += 1\n\n    return bow_matrix, vocab\n\n\n\n# #----------bow方法--------------\n# # 训练集（建立词表）\n# X_train_bow, vocab = build_bow(train_texts)\n\n# # 验证集（用训练集词表，不新建）\n# X_valid_bow, _ = build_bow(valid_texts, vocab)\n\n# # 测试集（同理）\n# X_test_bow, _ = build_bow(test_df[\"text\"], vocab)\n\n#------------n-gram方法------------------\nX_train_bow, vocab = build_ngram_bow(train_texts, n=2)\nX_valid_bow, _ = build_ngram_bow(valid_texts, n=2, vocab=vocab)\nX_test_bow, _  = build_ngram_bow(test_df[\"text\"], n=2, vocab=vocab)\n\n\n\n\n#标签转化为向量\ny_train_tensor = torch.tensor(train_labels.values, dtype=torch.long)\ny_valid_tensor = torch.tensor(valid_labels.values, dtype=torch.long)\ny_test_tensor = torch.tensor(test_df[\"label\"].values, dtype=torch.long)\n\n#检查一下\nprint(\"训练集:\", X_train_bow.shape, y_train_tensor.shape)\nprint(\"验证集:\", X_valid_bow.shape, y_valid_tensor.shape)\nprint(\"测试集:\", X_test_bow.shape, y_test_tensor.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-23T08:24:01.944004Z","iopub.status.idle":"2025-09-23T08:24:01.944267Z","shell.execute_reply.started":"2025-09-23T08:24:01.944137Z","shell.execute_reply":"2025-09-23T08:24:01.944147Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfrom torch.utils.data import DataLoader, TensorDataset\n\n#把特征 X_train_tensor 和标签 y_train_tensor 打包在一起\ntrain_dataset = TensorDataset(X_train_bow, y_train_tensor)\nvalid_dataset = TensorDataset(X_valid_bow, y_valid_tensor)\ntest_dataset = TensorDataset(X_test_bow, y_test_tensor)\n\n#为数据集划分batch，按batch读取TensorDataset\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=32)\ntest_loader = DataLoader(test_dataset, batch_size=32)\n\n#前向传播\n# def forward(X, W, b):\n#     logits = X @ W + b  #线性层\n#     probs = torch.softmax(logits, dim=1)  #在类别数量维度上做softmax\n#     return probs\n# --------- 前向传播 ----------\ndef forward(X, W, b, use_softmax=True):\n    logits = X @ W + b\n    if use_softmax:\n        probs = torch.softmax(logits, dim=1)  # 分类任务常用\n        return probs\n    else:\n        return logits   # 有些损失（比如 Hinge）直接用 logits\n\n\n#损失函数\ndef cross_entropy_loss(probs, y):\n    batch_size = y.size(0) #y为真实标签\n    real_probs = probs[range(batch_size), y] + 1e-9 #从每一行的概率分布里，挑出真实类别对应的概率\n    loss = -torch.log(real_probs).mean() #计算batch里面每一行的loss，并求均值\n    return loss\ndef mse_loss(probs, y, num_classes):\n    y_onehot = torch.zeros_like(probs)\n    y_onehot[range(y.size(0)), y] = 1.0\n    loss = ((probs - y_onehot) ** 2).mean()\n    return loss\ndef hinge_loss(probs, y, num_classes):\n    y_onehot = torch.zeros_like(probs)\n    y_onehot[range(y.size(0)), y] = 1.0\n    \n    correct_class_scores = (probs * y_onehot).sum(dim=1, keepdim=True)\n    margins = torch.clamp(probs - correct_class_scores + 1.0, min=0)\n    margins[range(y.size(0)), y] = 0  # 正确类别不算在 margin 里\n    loss = margins.mean()\n    return loss\n\n\n#计算准确率\ndef accuracy(probs, y):\n    preds = probs.argmax(dim=1)\n    return (preds==y).float().mean().item()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-23T08:24:01.945354Z","iopub.status.idle":"2025-09-23T08:24:01.945730Z","shell.execute_reply.started":"2025-09-23T08:24:01.945543Z","shell.execute_reply":"2025-09-23T08:24:01.945558Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!pip install /kaggle/input/tensorboardx/tensorboardx-2.6.4-py3-none-any.whl\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-23T08:24:01.947762Z","iopub.status.idle":"2025-09-23T08:24:01.948065Z","shell.execute_reply.started":"2025-09-23T08:24:01.947928Z","shell.execute_reply":"2025-09-23T08:24:01.947944Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#import tensorboardX\n#print(tensorboardX.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-23T08:24:01.949263Z","iopub.status.idle":"2025-09-23T08:24:01.949657Z","shell.execute_reply.started":"2025-09-23T08:24:01.949430Z","shell.execute_reply":"2025-09-23T08:24:01.949449Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#from tensorboardX import SummaryWriter\n\n#writer = SummaryWriter()\n\n#%load_ext tensorboard\n\n#%load_ext tensorboard","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-23T08:24:01.951787Z","iopub.status.idle":"2025-09-23T08:24:01.952069Z","shell.execute_reply.started":"2025-09-23T08:24:01.951941Z","shell.execute_reply":"2025-09-23T08:24:01.951957Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#初始化参数\n\n#行数=词表大小 列数=标签种类的个数\nW_row = X_train_bow.shape[1]\nW_col = len(set(train_labels))\n\nW = torch.randn(W_row, W_col, dtype=torch.float32, requires_grad=True)\nb = torch.zeros(W_col, dtype=torch.float32, requires_grad=True)\n\nnum_classes = W_col\nlr = 0.1\n\nprint(f\"共有 {num_classes} 种标签\")\n# 初始化列表\ntrain_losses = []\nvalid_losses = []\ntrain_accs = []\nvalid_accs = []\n\nnum_epochs = 15\nfor epoch in range(num_epochs):\n\n    # ----------训练---------\n    if W.grad is not None:\n        W.grad.zero_()\n    if b.grad is not None:\n        b.grad.zero_()\n    train_loss = 0\n    train_acc = 0\n    for X_batch, y_batch in train_loader:\n        batch_size = X_batch.size(0)\n        probs = forward(X_batch, W, b, use_softmax=True)\n        # loss = cross_entropy_loss(probs, y_batch)\n        loss = mse_loss(probs, y_batch, num_classes)\n        #loss = hinge_loss(probs, y_batch, num_classes)\n\n\n\n        loss.backward()  # 反向传播\n\n        # 参数更新\n        with torch.no_grad():\n            W -= lr * W.grad\n            b -= lr * b.grad\n\n        train_loss += loss.item() * batch_size\n        train_acc += accuracy(probs, y_batch) * batch_size\n\n    # ------验证--------\n    valid_loss = 0\n    valid_acc = 0\n    with torch.no_grad():\n        for X_batch, y_batch in valid_loader:\n            batch_size = X_batch.size(0)\n            probs = forward(X_batch, W, b, use_softmax=True)\n            # loss = cross_entropy_loss(probs, y_batch)\n              loss = mse_loss(probs, y_batch, num_classes)\n            #loss = hinge_loss(probs, y_batch, num_classes)\n\n\n            valid_loss += loss.item() * batch_size\n            valid_acc += accuracy(probs, y_batch) * batch_size\n\n    # 计算平均值\n    train_loss /= len(train_dataset)\n    valid_loss /= len(valid_dataset)\n    train_acc /= len(train_dataset)\n    valid_acc /= len(valid_dataset)\n\n    # 保存到列表\n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n    train_accs.append(train_acc)\n    valid_accs.append(valid_acc)\n\n    print(f\"Epoch {epoch+1}/{num_epochs} \"\n          f\"Train Loss: {train_loss:.4f} \"\n          f\"Train Acc: {train_acc:.4f} \"\n          f\"Valid Loss: {valid_loss:.4f} \"\n          f\"Valid Acc: {valid_acc:.4f}\")\n\n#-----------测试-------------\ntest_loss = 0;\ntest_acc = 0;\nwith torch.no_grad():\n    for X_batch, y_batch in test_loader:\n        batch_size = X_batch.size(0)\n        probs = forward(X_batch, W, b, use_softmax=True)\n        #loss = cross_entropy_loss(probs, y_batch)\n        loss = mse_loss(probs, y_batch, num_classes)\n        # loss = hinge_loss(probs, y_batch, num_classes)\n\n\n        test_loss += loss.item() * batch_size\n        test_acc += accuracy(probs, y_batch) * batch_size\n\ntest_loss /= len(test_dataset)\ntest_acc /= len(test_dataset)\n\nprint(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-23T08:24:01.953265Z","iopub.status.idle":"2025-09-23T08:24:01.953675Z","shell.execute_reply.started":"2025-09-23T08:24:01.953461Z","shell.execute_reply":"2025-09-23T08:24:01.953477Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#-------------结果可视化---------------\n\nimport matplotlib.pyplot as plt\n\nepochs = range(1, num_epochs + 1)\n\nplt.figure(figsize=(10,4))\nplt.subplot(1,2,1)\nplt.plot(epochs, train_losses, label='Train Loss', marker='o')\nplt.plot(epochs, valid_losses, label='Valid Loss', marker='o')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Loss Curve')\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(epochs, train_accs, label='Train Acc', marker='o')\nplt.plot(epochs, valid_accs, label='Valid Acc', marker='o')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Accuracy Curve')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-23T08:24:01.954650Z","iopub.status.idle":"2025-09-23T08:24:01.954939Z","shell.execute_reply.started":"2025-09-23T08:24:01.954802Z","shell.execute_reply":"2025-09-23T08:24:01.954816Z"}},"outputs":[],"execution_count":null}]}