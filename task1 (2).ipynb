{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12773518,"sourceType":"datasetVersion","datasetId":8075275},{"sourceId":12773525,"sourceType":"datasetVersion","datasetId":8075280},{"sourceId":13116883,"sourceType":"datasetVersion","datasetId":8309178}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport torch\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#读入数据\nimport pandas as pd\ntrain_df = pd.read_csv(\"/kaggle/input/train-tsv/new_train.tsv\",\n                       sep=\"\\t\", names=['text', 'label']) \ntest_df = pd.read_csv(\"/kaggle/input/train-tsv/new_train.tsv\", \n                       sep='\\t', names=['text', 'label'])\ntrain_df.head()\ntest_df.head()\n\n#划分数据集\nfrom sklearn.model_selection import train_test_split\ntrain_texts, valid_texts, train_labels, valid_labels = train_test_split(\n    train_df[\"text\"], train_df[\"label\"], test_size=0.2, random_state=42\n)\n\n#文本预处理\nimport re\ndef preprocess_text(text):\n    text = text.lower()\n    text = re.sub(r'[^a-z\\s]', '', text)\n    return text.split()\n\n#文本向量化--bow方法\nvocab = {}\ntokenized_sentences = []\n\ndef build_bow(sentences):\n    vocab = {} #vocab中单词的索引可用于后续填充bow矩阵\n    tokenized_sentences = []\n    for sent in sentences:\n        tokens = preprocess_text(sent)\n        tokenized_sentences.append(tokens)\n        for word in tokens:\n            if word not in vocab:\n                vocab[word] = len(vocab)\n\n    bow_matrix = torch.zeros(len(sentences), len(vocab), dtype=torch.float32)\n        \n    for i, token in enumerate(tokenized_sentences):\n        for word in tokens:\n            if word in vocab:\n                bow_matrix[i][vocab[word]] += 1\n\n    return bow_matrix, vocab\n#--------检验函数是否正确---------\nsentences = [\n    \"This movie is great!\",\n    \"This movie is bad...\",\n    \"I love this film.\"\n]\n\nX_bow, vocab = build_bow(sentences)\n\nprint(\"词表:\", vocab)\nprint(\"BoW 矩阵:\\n\", X_bow)\n#-------------------------------\n\n\n#n-gram方法\ndef generate_ngrams(tokens, n=2):\n    \"\"\"\n    从分词列表生成 n-grams，例如 bigram (n=2)\n    ['this', 'is', 'good'] -> ['this is', 'is good']\n    \"\"\"\n    if len(tokens) < n:\n        return []\n    ngrams = zip(*[tokens[i:] for i in range(n)])\n    return [\" \".join(ngram) for ngram in ngrams]\n    \ndef build_ngram_bow(sentences, n=2):\n    vocab = {}\n    tokenized_sentences = []\n    #构建词表\n    for sent in sentences:\n        tokens = preprocess_text(sent)\n        ngrams = generate_ngrams(tokens, n)\n        features = tokens + ngrams\n        tokenized_sentences.append(features)\n        for word in features:\n            if word not in vocab:\n                vocab[word] = len(vocab)\n    #初始化bow矩阵\n    bow_matrix = torch.zeros(len(sentences), len(vocab), dtype=torch.float32)\n    #填充bow矩阵\n    for i, features in enumerate(tokenized_sentences):\n            bow_matrix[i][vocab[word]] += 1\n\n    return bow_matrix, vocab \n#--------检验函数是否正确---------\nsentences = [\n    \"This movie is great!\",\n    \"This movie is bad...\",\n    \"I love this film.\"\n]\n\nX_bow, vocab = build_ngram_bow(sentences)\n\nprint(\"词表:\", vocab)\nprint(\"BoW 矩阵:\\n\", X_bow)\n#-------------------------------\n\n#文本正式向量化\nX_train_bow, vocab = build_bow(train_texts)#训练集\n\n#测试集\nX_valid_bow = torch.zeros(len(valid_texts), len(vocab), dtype=torch.float32)\nfor i, sent in enumerate(valid_texts):\n    tokens = preprocess_text(sent)\n    for word in tokens:\n        if word in vocab:\n            X_valid_bow[i][vocab[word]] += 1\n\n#测试集\nX_test_bow = torch.zeros(len(test_df[\"text\"]), len(vocab), dtype=torch.float32)\nfor i, sent in enumerate(test_df[\"text\"]):\n    tokens = preprocess_text(sent)\n    for word in tokens:\n        if word in vocab:\n            X_test_bow[i][vocab[word]] += 1\n\n#标签转化为向量\ny_train_tensor = torch.tensor(train_labels.values, dtype=torch.long)\ny_valid_tensor = torch.tensor(valid_labels.values, dtype=torch.long)\ny_test_tensor = torch.tensor(test_df[\"label\"].values, dtype=torch.long)\n\n#检查一下\nprint(\"训练集:\", X_train_bow.shape, y_train_tensor.shape)\nprint(\"验证集:\", X_valid_bow.shape, y_valid_tensor.shape)\nprint(\"测试集:\", X_test_bow.shape, y_test_tensor.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfrom torch.utils.data import DataLoader, TensorDataset\n\n#把特征 X_train_tensor 和标签 y_train_tensor 打包在一起\ntrain_dataset = TensorDataset(X_train_bow, y_train_tensor)\nvalid_dataset = TensorDataset(X_valid_bow, y_valid_tensor)\ntest_dataset = TensorDataset(X_test_bow, y_test_tensor)\n\n#为数据集划分batch，按batch读取TensorDataset\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=64)\ntest_loader = Dataloader(test_dataset, batch_size=64)\n\n#前向传播\ndef forward(X, W, b):\n    logits = X @ W + b  #线性层\n    probs = torch.softmax(logits, dim=1)  #在类别数量维度上做softmax\n    return probs\n\n#损失函数\ndef cross_entropy_loss(probs, y):\n    batch_size = y.size(0) #y为真实标签\n    real_probs = probs[range(batch_size), y] + 1e-9 #从每一行的概率分布里，挑出真实类别对应的概率\n    loss = -torch.log(real_probs).mean() #计算batch里面每一行的loss，并求均值\n    return loss\n\n#计算准确率\ndef accuracy(probs, y):\n    preds = probs.argmax(dim=1)\n    return (preds==y).float().mean().item()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!pip install /kaggle/input/tensorboardx/tensorboardx-2.6.4-py3-none-any.whl\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#import tensorboardX\n#print(tensorboardX.__version__)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#from tensorboardX import SummaryWriter\n\n#writer = SummaryWriter()\n\n#%load_ext tensorboard\n\n#%load_ext tensorboard","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#初始化参数\n\n#行数=词表大小 列数=标签种类的个数\nW_row = X_train_bow.shape[1]\nW_col = len(set(train_labels))\n\nW = torch.randn(W_row, W_col, dtype=torch.float32, requires_grad=True)\nb = torch.zeros(W_col, dtype=torch.float32, requires_grad=True)\n\nnum_classes = W_col\nlr = 0.001\n\nprint(f\"共有 {num_classes} 种标签\")\n# 初始化列表\ntrain_losses = []\nvalid_losses = []\ntrain_accs = []\nvalid_accs = []\n\nnum_epochs = 50\nfor epoch in range(num_epochs):\n    W.grad = None\n    b.grad = None  # 清空梯度\n\n    # ----------训练---------\n    train_loss = 0\n    train_acc = 0\n    for X_batch, y_batch in train_loader:\n        batch_size = X_batch.size(0)\n        probs = forward(X_batch, W, b)\n        loss = cross_entropy_loss(probs, y_batch)\n\n        loss.backward()  # 反向传播\n\n        # 参数更新\n        with torch.no_grad():\n            W -= lr * W.grad\n            b -= lr * b.grad\n            W.grad.zero_()\n            b.grad.zero_()\n\n        train_loss += loss.item() * batch_size\n        train_acc += accuracy(probs, y_batch) * batch_size\n\n    # ------验证--------\n    valid_loss = 0\n    valid_acc = 0\n    with torch.no_grad():\n        for X_batch, y_batch in valid_loader:\n            batch_size = X_batch.size(0)\n            probs = forward(X_batch, W, b)\n            loss = cross_entropy_loss(probs, y_batch)\n            valid_loss += loss.item() * batch_size\n            valid_acc += accuracy(probs, y_batch) * batch_size\n\n    # 计算平均值\n    train_loss /= len(train_dataset)\n    valid_loss /= len(valid_dataset)\n    train_acc /= len(train_dataset)\n    valid_acc /= len(valid_dataset)\n\n    # 保存到列表\n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n    train_accs.append(train_acc)\n    valid_accs.append(valid_acc)\n\n    print(f\"Epoch {epoch+1}/{num_epochs} \"\n          f\"Train Loss: {train_loss:.4f} \"\n          f\"Train Acc: {train_acc:.4f} \"\n          f\"Valid Loss: {valid_loss:.4f} \"\n          f\"Valid Acc: {valid_acc:.4f}\")\n\n#-----------测试-------------\ntest_loss = 0;\ntest_acc = 0;\nwith torch.no_grad():\n    for X_batch, y_batch in test_loader:\n        batch_size = X_batch.size()\n        probs = forward(X_batch, W, b)\n        loss = cross_entropy_loss(probs, y_batch)\n        test_loss += loss.item() * batch_size\n        test_acc += accuracy(probs, y_batch) * batch_size\n\ntest_loss /= len(test_dataset)\ntest_acc /= len(test_dataset)\n\nprint(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#-------------结果可视化---------------\n\nimport matplotlib.pyplot as plt\n\nepochs = range(1, num_epochs + 1)\n\nplt.figure(figsize=(10,4))\nplt.subplot(1,2,1)\nplt.plot(epochs, train_losses, label='Train Loss', marker='o')\nplt.plot(epochs, valid_losses, label='Valid Loss', marker='o')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Loss Curve')\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(epochs, train_accs, label='Train Acc', marker='o')\nplt.plot(epochs, valid_accs, label='Valid Acc', marker='o')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Accuracy Curve')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}