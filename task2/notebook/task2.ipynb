{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12773518,"sourceType":"datasetVersion","datasetId":8075275},{"sourceId":12773525,"sourceType":"datasetVersion","datasetId":8075280},{"sourceId":13116883,"sourceType":"datasetVersion","datasetId":8309178},{"sourceId":13158414,"sourceType":"datasetVersion","datasetId":8337414},{"sourceId":13170781,"sourceType":"datasetVersion","datasetId":8346042}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-23T15:30:01.552583Z","iopub.execute_input":"2025-09-23T15:30:01.552826Z","iopub.status.idle":"2025-09-23T15:30:01.558371Z","shell.execute_reply.started":"2025-09-23T15:30:01.552810Z","shell.execute_reply":"2025-09-23T15:30:01.557646Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =====================\n# 导入依赖\n# =====================\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nimport pandas as pd\nimport re\nfrom sklearn.model_selection import train_test_split\nimport torch.nn.functional as F\n\n\n\n# =====================\n# 配置区（实验变量宏定义）\n# =====================\n\n# 数据路径\nTRAIN_PATH = \"/kaggle/input/train-tsv/new_train.tsv\"\nTEST_PATH  = \"/kaggle/input/test-tsv/new_test.tsv\"\n\n# 数据预处理\nMAX_LEN = 100              # 句子最大长度\nVOCAB_MIN_FREQ = 1         # 词表最小词频（这里没用到，可扩展）\n\n# 模型参数\nEMBED_DIM = 100\nUSE_GLOVE = True\nGLOVE_PATH = \"/kaggle/input/glove-100d/wiki_giga_2024_100_MFT20_vectors_seed_2024_alpha_0.75_eta_0.05.050_combined.txt\"\nKERNEL_SIZES = [3, 4, 5]\nNUM_CHANNELS = 50\nDROPOUT = 0.5\nMODEL_TYPE = \"DeepCNN\"   # 可选: \"CNN\", \"RNN\", \"TRANSFORMER\", \"DeepCNN\"\n\n# 训练参数\nNUM_CLASSES = 5\nBATCH_SIZE = 32\nNUM_EPOCHS = 15\nLEARNING_RATE = 2e-4\nLOSS_FUNCTION = \"CrossEntropy\"   # 可选: \"CrossEntropy\", \"MSE\"\nOPTIMIZER = \"Adam\"               # 可选: \"Adam\", \"SGD\"\n\n# 设备\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\n\n# =====================\n# 1. 数据读取与预处理\n# =====================\ndef preprocess(text):\n    text = text.lower()\n    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n    return text.split()\n\ntrain_df = pd.read_csv(TRAIN_PATH, sep=\"\\t\", names=[\"text\", \"label\"])\ntest_df  = pd.read_csv(TEST_PATH,  sep=\"\\t\", names=[\"text\", \"label\"])\ntrain_df[\"tokens\"] = train_df[\"text\"].apply(preprocess)\ntest_df[\"tokens\"]  = test_df[\"text\"].apply(preprocess)\n\n# 构建词表\nall_tokens = [tok for tokens in train_df[\"tokens\"] for tok in tokens]\nvocab = {\"<PAD>\": 0, \"<UNK>\": 1}\nfor tok in all_tokens:\n    if tok not in vocab:\n        vocab[tok] = len(vocab)\nvocab_size = len(vocab)\n\n# tokens → ids\ndef tokens_to_ids(tokens, vocab, max_len=MAX_LEN):\n    ids = [vocab.get(tok, 1) for tok in tokens]  # 1 = <UNK>\n    ids = ids[:max_len]\n    ids += [0] * (max_len - len(ids))            # PAD\n    return ids\n\ntrain_ids = [tokens_to_ids(toks, vocab) for toks in train_df[\"tokens\"]]\ntest_ids  = [tokens_to_ids(toks, vocab) for toks in test_df[\"tokens\"]]\n\n# 划分验证集\ntrain_ids, valid_ids, train_labels, valid_labels = train_test_split(\n    train_ids, train_df[\"label\"], test_size=0.2, random_state=42\n)\n\ntrain_dataset = TensorDataset(torch.tensor(train_ids), torch.tensor(train_labels.values))\nvalid_dataset = TensorDataset(torch.tensor(valid_ids), torch.tensor(valid_labels.values))\ntest_dataset  = TensorDataset(torch.tensor(test_ids),  torch.tensor(test_df[\"label\"].values))\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE)\ntest_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE)\n\n# =====================\n# 2. 模型定义\n# =====================\n\n# (可选) GloVe 初始化\ndef load_glove_embeddings(glove_path, vocab, embed_dim):\n    embeddings = torch.randn(len(vocab), embed_dim) * 0.6\n    embeddings[0] = torch.zeros(embed_dim)   # <PAD> 用 0 向量\n\n    with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            parts = line.strip().split()\n            word = parts[0]\n            vec = parts[1:]\n            if len(vec) != embed_dim:\n                continue\n            if word in vocab:\n                embeddings[vocab[word]] = torch.tensor([float(x) for x in vec])\n    return embeddings\n\n\n# CNN\nclass TextCNN(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_classes):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        if USE_GLOVE:\n            glove_matrix = load_glove_embeddings(GLOVE_PATH, vocab, embed_dim)\n            self.embedding = nn.Embedding.from_pretrained(glove_matrix, freeze=False, padding_idx=0)\n        else:\n            self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n\n        self.convs = nn.ModuleList([\n            nn.Conv1d(in_channels=embed_dim, out_channels=NUM_CHANNELS, kernel_size=k)\n            for k in KERNEL_SIZES\n        ])\n        self.dropout = nn.Dropout(DROPOUT)\n        self.fc = nn.Linear(NUM_CHANNELS * len(KERNEL_SIZES), num_classes)\n\n    def forward(self, x):\n        x = self.embedding(x)           # (batch, seq_len, embed_dim)\n        x = x.permute(0, 2, 1)          # (batch, embed_dim, seq_len)\n        conv_outs = [torch.relu(conv(x)) for conv in self.convs]\n        pooled = [torch.max(out, dim=2)[0] for out in conv_outs]\n        features = torch.cat(pooled, dim=1)\n        return self.fc(self.dropout(features))\n\n# RNN (简单 LSTM)\nclass TextRNN(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_classes):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        if USE_GLOVE:\n            glove_matrix = load_glove_embeddings(GLOVE_PATH, vocab, embed_dim)\n            self.embedding = nn.Embedding.from_pretrained(glove_matrix, freeze=False, padding_idx=0)\n        else:\n            self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.rnn = nn.LSTM(embed_dim, hidden_size=128, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(128 * 2, num_classes)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        _, (h, _) = self.rnn(x)\n        features = torch.cat((h[-2], h[-1]), dim=1)  # 拼接双向\n        return self.fc(features)\n\n# Transformer (简单版 Encoder)\nclass TextTransformer(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_classes, num_heads=4, num_layers=2):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        if USE_GLOVE:\n            glove_matrix = load_glove_embeddings(GLOVE_PATH, vocab, embed_dim)\n            self.embedding = nn.Embedding.from_pretrained(glove_matrix, freeze=False, padding_idx=0)\n        else:\n            self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads)\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.fc = nn.Linear(embed_dim, num_classes)\n\n    def forward(self, x):\n        x = self.embedding(x)                # (batch, seq_len, embed_dim)\n        x = x.permute(1, 0, 2)               # (seq_len, batch, embed_dim)\n        out = self.transformer(x)            # (seq_len, batch, embed_dim)\n        features = out.mean(dim=0)           # 平均池化\n        return self.fc(features)\n\nclass DeepCNN(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_classes, pretrained_embeddings=None,\n                 kernel_sizes=[3,4,5], num_channels=100, dropout=0.5):\n        super(DeepCNN, self).__init__()\n        \n        # Embedding层\n        if pretrained_embeddings is not None:\n            self.embedding = nn.Embedding.from_pretrained(\n                torch.tensor(pretrained_embeddings, dtype=torch.float),\n                freeze=False  # 允许微调\n            )\n        else:\n            self.embedding = nn.Embedding(vocab_size, embed_dim)\n        \n        self.convs = nn.ModuleList()\n        for ks in kernel_sizes:\n            # 堆叠两层卷积（增加深度）\n            conv_block = nn.Sequential(\n                nn.Conv1d(embed_dim, num_channels, kernel_size=ks, padding=ks//2),\n                nn.ReLU(),\n                nn.Conv1d(num_channels, num_channels, kernel_size=ks, padding=ks//2),\n                nn.ReLU(),\n                nn.MaxPool1d(kernel_size=2)  # 下采样，增加表达能力\n            )\n            self.convs.append(conv_block)\n        \n        # 全连接层\n        self.fc = nn.Linear(num_channels * len(kernel_sizes), num_classes)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # x: (batch, seq_len)\n        embedded = self.embedding(x)           # (batch, seq_len, embed_dim)\n        embedded = embedded.permute(0, 2, 1)   # (batch, embed_dim, seq_len) for Conv1d\n\n        conv_outs = []\n        for conv in self.convs:\n            c = conv(embedded)  # (batch, num_channels, seq_len//2)\n            pooled = F.adaptive_max_pool1d(c, 1).squeeze(-1)  # 全局池化到固定维度\n            conv_outs.append(pooled)\n\n        out = torch.cat(conv_outs, dim=1)\n        out = self.dropout(out)\n        return self.fc(out)\n\n# 模型选择\nif MODEL_TYPE == \"CNN\":\n    model = TextCNN(vocab_size, EMBED_DIM, NUM_CLASSES)\nelif MODEL_TYPE == \"RNN\":\n    model = TextRNN(vocab_size, EMBED_DIM, NUM_CLASSES)\nelif MODEL_TYPE == \"TRANSFORMER\":\n    model = TextTransformer(vocab_size, EMBED_DIM, NUM_CLASSES)\nelif MODEL_TYPE == \"DeepCNN\":\n    model = DeepCNN(\n    vocab_size=vocab_size,\n    embed_dim=EMBED_DIM,\n    num_classes=NUM_CLASSES,\n    pretrained_embeddings=None,   # 如果想用 GloVe，就传 numpy 矩阵\n    kernel_sizes=[3,4,5],\n    num_channels=100,\n    dropout=0.5\n).to(DEVICE)\nelse:\n    raise ValueError(\"Unknown MODEL_TYPE\")\n\nmodel = model.to(DEVICE)\n\n# =====================\n# 3. 损失函数 & 优化器\n# =====================\nif LOSS_FUNCTION == \"CrossEntropy\":\n    criterion = nn.CrossEntropyLoss()\nelif LOSS_FUNCTION == \"MSE\":\n    criterion = nn.MSELoss()\n\nif OPTIMIZER == \"Adam\":\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\nelif OPTIMIZER == \"SGD\":\n    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n\nimport matplotlib.pyplot as plt\n\n# =====================\n# 4. 训练 & 验证（保存曲线数据）\n# =====================\nhistory = {\n    \"train_loss\": [],\n    \"train_acc\": [],\n    \"val_loss\": [],\n    \"val_acc\": []\n}\n\ndef evaluate(loader, criterion=None):\n    \"\"\"评估函数，可以返回 acc，也可以计算 loss\"\"\"\n    model.eval()\n    correct, total, total_loss = 0, 0, 0\n    with torch.no_grad():\n        for x, y in loader:\n            x, y = x.to(DEVICE), y.to(DEVICE)\n            preds = model(x)\n            if LOSS_FUNCTION == \"MSE\":\n                y_onehot = F.one_hot(y, num_classes=NUM_CLASSES).float()\n                loss = criterion(preds, y_onehot)\n            else:\n                loss = criterion(preds, y)\n            total_loss += loss.item()\n            pred_labels = preds.argmax(dim=1)\n            correct += (pred_labels == y).sum().item()\n            total += y.size(0)\n    acc = correct / total\n    avg_loss = total_loss / len(loader) if criterion else 0\n    return acc, avg_loss\n\nfor epoch in range(NUM_EPOCHS):\n    model.train()\n    total_loss, correct, total = 0, 0, 0\n\n    for x, y in train_loader:\n        x, y = x.to(DEVICE), y.to(DEVICE)\n        optimizer.zero_grad()\n        preds = model(x)\n        if LOSS_FUNCTION == \"MSE\":\n            y_onehot = F.one_hot(y, num_classes=NUM_CLASSES).float()\n            loss = criterion(preds, y_onehot)\n        else:\n            loss = criterion(preds, y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        pred_labels = preds.argmax(dim=1)\n        correct += (pred_labels == y).sum().item()\n        total += y.size(0)\n\n    # 计算 train 的平均 loss/acc\n    train_loss = total_loss / len(train_loader)\n    train_acc = correct / total\n\n    # 计算 valid 的 loss/acc\n    val_acc, val_loss = evaluate(valid_loader, criterion)\n\n    # 保存记录\n    history[\"train_loss\"].append(train_loss)\n    history[\"train_acc\"].append(train_acc)\n    history[\"val_loss\"].append(val_loss)\n    history[\"val_acc\"].append(val_acc)\n\n    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | \"\n          f\"Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f} | \"\n          f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}\")\n\n# =====================\n# 5. 可视化训练曲线\n# =====================\nplt.figure(figsize=(12,5))\n\n# Loss 曲线\nplt.subplot(1,2,1)\nplt.plot(history[\"train_loss\"], label=\"Train Loss\")\nplt.plot(history[\"val_loss\"], label=\"Val Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training & Validation Loss\")\nplt.legend()\n\n# Acc 曲线\nplt.subplot(1,2,2)\nplt.plot(history[\"train_acc\"], label=\"Train Acc\")\nplt.plot(history[\"val_acc\"], label=\"Val Acc\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Training & Validation Accuracy\")\nplt.legend()\n\nplt.show()\n\n# =====================\n# 6. 测试集结果\n# =====================\ntest_acc, test_loss = evaluate(test_loader, criterion)\nprint(f\"Final Test Loss={test_loss:.4f}, Test Acc={test_acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T01:05:09.527800Z","iopub.execute_input":"2025-09-26T01:05:09.528506Z","iopub.status.idle":"2025-09-26T01:05:28.180213Z","shell.execute_reply.started":"2025-09-26T01:05:09.528479Z","shell.execute_reply":"2025-09-26T01:05:28.179416Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\n# ===== GloVe 加载函数 =====\ndef load_glove_vocab(path, embed_dim):\n    glove = {}\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            parts = line.strip().split()\n            word, vec = parts[0], parts[1:]\n            if len(vec) != embed_dim:\n                continue\n            glove[word] = 1\n    print(f\"GloVe 文件中共有 {len(glove)} 个词\")\n    return glove\n\n# ===== 验证 OOV =====\nglove_vocab = load_glove_vocab(GLOVE_PATH, EMBED_DIM)\n\nfound, not_found = 0, 0\nfor word in vocab.keys():\n    if word in glove_vocab:\n        found += 1\n    else:\n        not_found += 1\n\nprint(f\"词表大小: {len(vocab)}\")\nprint(f\"在 GloVe 中找到的词: {found}\")\nprint(f\"没找到 (OOV) 的词: {not_found}\")\nprint(f\"OOV率: {not_found / len(vocab):.2%}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T15:05:40.017819Z","iopub.execute_input":"2025-09-24T15:05:40.018473Z","iopub.status.idle":"2025-09-24T15:05:50.139346Z","shell.execute_reply.started":"2025-09-24T15:05:40.018446Z","shell.execute_reply":"2025-09-24T15:05:50.138486Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import Counter\ncounts = Counter(train_df[\"label\"])\nprint(\"label counts:\", counts)\nmost_common_acc = max(counts.values()) / sum(counts.values())\nprint(\"Majority class accuracy (baseline):\", most_common_acc)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T11:14:58.830142Z","iopub.execute_input":"2025-09-25T11:14:58.830952Z","iopub.status.idle":"2025-09-25T11:14:58.836805Z","shell.execute_reply.started":"2025-09-25T11:14:58.830918Z","shell.execute_reply":"2025-09-25T11:14:58.836052Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import Counter\n\nmodel.eval()\nall_preds = []\nwith torch.no_grad():\n    for X, y in valid_loader:\n        preds = model(X.to(DEVICE)).argmax(1).cpu().numpy()\n        all_preds.extend(preds)\n\nprint(\"Predicted label distribution:\", Counter(all_preds))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T11:16:15.114577Z","iopub.execute_input":"2025-09-25T11:16:15.114816Z","iopub.status.idle":"2025-09-25T11:16:15.261669Z","shell.execute_reply.started":"2025-09-25T11:16:15.114798Z","shell.execute_reply":"2025-09-25T11:16:15.261060Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\ncv = CountVectorizer(max_features=5000)\nX = cv.fit_transform(train_df[\"text\"])\ny = train_df[\"label\"]\n\nXt, Xv, yt, yv = train_test_split(X, y, test_size=0.2, random_state=42)\nlr = LogisticRegression(max_iter=2000).fit(Xt, yt)\nprint(\"BoW Logistic acc:\", lr.score(Xv, yv))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T11:17:14.263757Z","iopub.execute_input":"2025-09-25T11:17:14.264125Z","iopub.status.idle":"2025-09-25T11:17:17.818887Z","shell.execute_reply.started":"2025-09-25T11:17:14.264103Z","shell.execute_reply":"2025-09-25T11:17:17.816293Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#--------------使用经过大量文本预训练的bert------------\n#发现其表现的确都优于原来实现的三种模型\nimport torch\nimport pandas as pd\nimport re\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom torch.optim import AdamW\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom tqdm import tqdm\n\n# ==============\n# 参数\n# ==============\nTRAIN_PATH = \"/kaggle/input/train-tsv/new_train.tsv\"\nTEST_PATH  = \"/kaggle/input/test-tsv/new_test.tsv\"\nMAX_LEN = 100\nBATCH_SIZE = 32\nNUM_EPOCHS = 3\nLEARNING_RATE = 1e-5\nNUM_CLASSES = 5\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# ==============\n# 1. 数据读取\n# ==============\ntrain_df = pd.read_csv(TRAIN_PATH, sep=\"\\t\", names=[\"text\", \"label\"])\ntest_df  = pd.read_csv(TEST_PATH,  sep=\"\\t\", names=[\"text\", \"label\"])\n\n# 划分训练/验证\ntrain_texts, valid_texts, train_labels, valid_labels = train_test_split(\n    train_df[\"text\"], train_df[\"label\"], test_size=0.2, random_state=42\n)\n\n# ==============\n# 2. 数据集类\n# ==============\nclass BertTextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len=MAX_LEN):\n        self.texts = list(texts)\n        self.labels = list(labels)\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        label = int(self.labels[idx])\n\n        encoding = self.tokenizer(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n\n        return {\n            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n            \"labels\": torch.tensor(label, dtype=torch.long)\n        }\n\n# ==============\n# 3. 模型 & DataLoader\n# ==============\ntokenizer = BertTokenizer.from_pretrained(\"/kaggle/input/bert-test\")  # 本地路径\nmodel = BertForSequenceClassification.from_pretrained(\n    \"/kaggle/input/bert-test\", num_labels=NUM_CLASSES\n).to(DEVICE)\n\ntrain_dataset = BertTextDataset(train_texts, train_labels, tokenizer)\nvalid_dataset = BertTextDataset(valid_texts, valid_labels, tokenizer)\ntest_dataset  = BertTextDataset(test_df[\"text\"], test_df[\"label\"], tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE)\ntest_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE)\n\noptimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n\n# ==============\n# 4. 训练函数\n# ==============\ndef train_epoch(model, loader, optimizer, device):\n    model.train()\n    total_loss = 0\n    for batch in tqdm(loader, desc=\"Training\"):\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n    return total_loss / len(loader)\n\n# ==============\n# 5. 验证函数\n# ==============\ndef eval_epoch(model, loader, device):\n    model.eval()\n    preds, labels_list = [], []\n    total_loss = 0\n    with torch.no_grad():\n        for batch in tqdm(loader, desc=\"Evaluating\"):\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"labels\"].to(device)\n\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            logits = outputs.logits\n\n            total_loss += loss.item()\n            preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n            labels_list.extend(labels.cpu().numpy())\n\n    acc = accuracy_score(labels_list, preds)\n    return total_loss / len(loader), acc\n\n# ==============\n# 6. 主训练循环\n# ==============\nfor epoch in range(NUM_EPOCHS):\n    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n    train_loss = train_epoch(model, train_loader, optimizer, DEVICE)\n    val_loss, val_acc = eval_epoch(model, valid_loader, DEVICE)\n\n    print(f\"Train Loss: {train_loss:.4f}\")\n    print(f\"Val   Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n\n# ==============\n# 7. 最终测试\n# ==============\ntest_loss, test_acc = eval_epoch(model, test_loader, DEVICE)\nprint(f\"\\nFinal Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}